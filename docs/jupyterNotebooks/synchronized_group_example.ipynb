{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitc2d01bbb5ec14380b37b83d30aac7c6a",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we showcase the SynchronizedGroup class that is useful to make a dataloader from multiple datasets\n",
    "\n",
    "from pioneer.das.api.platform import SynchronizedGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of the datasets (in a list). They all should have identical 'platform.yml' files to avoid conflits.\n",
    "datasets = [\n",
    "    '/nas/pixset/exportedDataset/20200706_143808_rec_dataset_berry_uquam_exported',\n",
    "    '/nas/pixset/exportedDataset/20200706_144800_rec_dataset_berry_uquam2_exported',\n",
    "    '/nas/pixset/exportedDataset/20200706_151313_rec_dataset_berry_uquam4_exported',\n",
    "]\n",
    "# Note that you also can put all datasets under a single directory and pass the path to that directory (not in a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading sensors: 100%|██████████| 9/9 [00:37<00:00,  4.14s/it]\n",
      "Synchronizing: 100%|██████████| 7/7 [00:00<00:00, 11.65it/s]\n",
      "Grouping synchronized platforms: 100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create the synchronizedGroup instance\n",
    "sg = SynchronizedGroup(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15018\n"
     ]
    }
   ],
   "source": [
    "# It works similarly to a Synchronized object. Its lenght is the sum of its parts.\n",
    "print(len(sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading sensors: 100%|██████████| 9/9 [00:11<00:00,  1.25s/it]\n",
      "Synchronizing: 100%|██████████| 7/7 [00:00<00:00, 44.48it/s]\n",
      "Grouping synchronized platforms: 100%|██████████| 3/3 [02:23<00:00, 47.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# The individual datasets are kept in a cache and only the first one is initialized by default.\n",
    "# Other datasets are loaded in the cache only when you try to access data from them.\n",
    "# If you want all datasets to be pre-loaded (better for random access dataloaders), you can do that: \n",
    "sg = SynchronizedGroup(datasets, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading sensors: 100%|██████████| 9/9 [00:02<00:00,  3.00it/s]\n",
      "Synchronizing: 100%|██████████| 7/7 [00:00<00:00, 25.94it/s]\n",
      "Grouping synchronized platforms: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]10169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can use the 'sync_labels', 'interp_labels' and 'tolerance_us' arguments to override the synchronization parameters.\n",
    "sg = SynchronizedGroup(datasets, tolerance_us=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10169\n"
     ]
    }
   ],
   "source": [
    "print(len(sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading sensors: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Synchronizing: 100%|██████████| 2/2 [00:00<00:00, 72.79it/s]\n",
      "Grouping synchronized platforms: 100%|██████████| 3/3 [00:00<00:00,  8.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# You can use the 'include' and 'ignore' arguments to change which sensors to use.\n",
    "# For example, if you only need the Pixell data with the 3d bounding boxes:\n",
    "sg = SynchronizedGroup(datasets, include=['pixell_bfc'], sync_labels=['*_ech','*_box3d-deepen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4414\n"
     ]
    }
   ],
   "source": [
    "print(len(sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}